Celery + RabbitMQ Explained Like a Conversation
==============================================

Why we need them
----------------
- Web requests should finish quickly. Heavy work (scraping, LLM calls, email) should move to the background.
- Celery is the background worker. RabbitMQ is the mailman that moves job messages around.
- With them we can accept a scan request instantly and do the long work somewhere else.

Core roles
----------
1. Producer: the FastAPI server. It creates a JSON message that says "run discovery for job 123".
2. Broker: RabbitMQ. It stores every message in a queue until a worker takes it.
3. Worker: a Celery process. It pulls a message, runs the Python function that is linked to that queue, and reports the result.
4. Result backend: Redis in our case. The worker drops task status (PENDING, STARTED, SUCCESS, FAILURE) plus return data here so the app can check progress.

How a message travels
---------------------
1. FastAPI calls `celery_app.send_task()` or `.delay()`.
2. Celery serializes arguments to JSON and publishes them to RabbitMQ.
3. RabbitMQ puts the message inside the queue name we chose (for example `scan.discovery`).
4. An idle worker listening to that queue takes the message, runs the Python function, and keeps working until it finishes.
5. The worker stores the outcome in Redis and acknowledges the message so RabbitMQ can delete it.

Why RabbitMQ?
-------------
- It is battle tested and can keep thousands of queued jobs in memory + disk.
- It filters jobs into different queues, which lets us throttle heavy stages separately.
- It supports acknowledgements, retries, dead letter exchanges, and routing keys. Celery already knows how to use those features.

Why Celery?
-----------
- Pure Python, integrates nicely with FastAPI.
- Handles retries, countdowns, chaining (run task A then B), chords (wait for many tasks), and a scheduler (beat).
- Gives the `celery worker` command that spawns processes/threads to execute tasks in parallel.

Worker basics
-------------
- Start: `celery -A app.platform.celery_app worker --loglevel=info --concurrency=4 -P solo`
- Concurrency = how many tasks run at the same time inside one worker process.
- Pool (`-P`) controls execution model. We use `solo` on Windows for stability.
- Stopping a worker safely lets it finish the current job before exiting.

Reliability knobs we use
------------------------
- `task_acks_late=True`: acknowledge only after the task succeeded so a crash makes RabbitMQ requeue it.
- `task_reject_on_worker_lost=True`: similar safety if the worker dies mid-task.
- `autoretry_for` on some tasks: Celery automatically retries transient errors.
- `task_time_limit=3600`: kill tasks that get stuck longer than one hour.

Celery Beat (scheduler)
-----------------------
- Beat is a lightweight scheduler that sends tasks at fixed times. Think "cron for Celery".
- We schedule the periodic scan checker once per hour. Beat sends a message to queue `celery`; normal workers pick it up.

Common RabbitMQ inspection commands
-----------------------------------
- `rabbitmqctl list_queues` to see queue sizes.
- `rabbitmqctl purge_queue scan.analysis` to empty a queue (careful!).
- Management UI (optional plugin) gives a dashboard where you can watch consumers, rate, and errors.

Common Celery inspection commands
---------------------------------
- `celery -A app.platform.celery_app status` shows connected workers.
- `celery -A app.platform.celery_app inspect active` lists tasks currently running.
- `celery -A app.platform.celery_app inspect scheduled` lists ETA or countdown tasks waiting in memory.

Mental model cheat sheet
------------------------
- FastAPI = receptionist who takes orders.
- RabbitMQ = mail room. Keeps envelopes (tasks) in bins (queues).
- Celery worker = employee who grabs an envelope, finishes the work, writes the result to Redis, then grabs another envelope.
- Redis = filing cabinet where the final report is saved for anyone who wants to read it.
